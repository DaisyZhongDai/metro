cd /s/jyang96
cd spark-1.4.1-bin-hadoop2.6
./bin/pyspark



from pyspark import SparkContext, kj
from pyspark.sql import SQLContext
from pyspark.sql.types import *


sqlContext = SQLContext(sc)


##------------------------tian-zi-fang-4.8-a2


taxi_a2_1 = sc.textFile("/project/nlp/soda/taxdata/part-00008",use_unicode=False).map(lambda col: col.split(",")).map(lambda
line:(line[0], line[2], line[7], float(line[8]), float(line[9])))

fieldName = ['ID', 'empty', 'send', 'longitude', 'lattitude']

field = [StructField("ID", StringType(), False),StructField('empty', StringType(), False),
         StructField('send', StringType(), False),StructField('longitude', DoubleType(), False),
         StructField('lattitude', DoubleType(), False)]

schema = StructType(field)


taxi_a2_2 = sqlContext.createDataFrame(taxi_a1_1, schema)

taxi_a2_2.show(1)
taxi_a2_2.take(1)


taxi_a2_3 = taxi_a2_2.filter( taxi_a2_2.longitude>121.460845)
taxi_a2_4 = taxi_a2_3.filter( taxi_a2_3.longitude<121.470845)
taxi_a2_5 = taxi_a2_4.filter( taxi_a2_4.lattitude>31.205884)
taxi_a2_6 = taxi_a2_5.filter( taxi_a2_5.lattitude<31.215884)

taxi_a2_6.show(10)

taxi_a2_6.count()
##661127
taxi_a2_6.map(lambda line: (line.ID, line.empty, line.send, line.longitude, line.lattitude)).coalesce(1,True).saveAsTextFile("/project/nlp/soda/taxdata/taxia2")


